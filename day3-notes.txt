	
---------------------------------------

kafka download:

wget https://archive.apache.org/dist/kafka/3.6.0/kafka_2.12-3.6.0.tgz --no-check-certificate


tar -xvf kafka_2.12-3.6.0.tgz

mkdir -p /tmp/zookeeper1
mkdir -p /tmp/zookeeper2
mkdir -p /tmp/zookeeper3

chmod 777 -R /tmp/zookeeper1
chmod 777 -R /tmp/zookeeper2
chmod 777 -R /tmp/zookeeper3

echo "1" >> /tmp/zookeeper1/myid


-------------------

echo "2" >>  /tmp/zookeeper2/myid

------------
echo "3" >> /tmp/zookeeper3/myid

-------------

cd /root/kafka_2.12-3.6.0/configcd

vi zookeeper1.properties

dataDir=/tmp/zookeeper1
clientPort=2181
admin.enableServer=true
admin.serverPort=8080
tickTime=2000
initLimit=5
syncLimit=2
server.1=localhost:2888:3888
server.2=localhost:2989:3088
server.3=localhost:3089:3188
autopurge.snapRetainCount=3
autopurge.purgeInterval=24


cp zookeeper1.properties zookeeper2.properties 
cp zookeeper1.properties zookeeper3.properties 

vi zookeeper2.properties 
dataDir=/tmp/zookeeper2
clientPort=2182
admin.enableServer=false

vi zookeeper3.properties 
dataDir=/tmp/zookeeper3
clientPort=2183
admin.enableServer=false
---------------------

To start the zookeeper 


cd /root/kafka_2.12-3.6.0/bin

nohup ./zookeeper-server-start.sh ../config/zookeeper1.properties &
nohup ./zookeeper-server-start.sh ../config/zookeeper2.properties &

nohup ./zookeeper-server-start.sh ../config/zookeeper3.properties &
-------------------------------------------------------
echo srvr |nc localhost 2181 | grep Mode
echo srvr |nc localhost 2182 | grep Mode
echo srvr |nc localhost 2183 | grep Mode
------------------------------------------
cd /root/kafka_2.12-3.6.0/config


vi server1.properties
broker.id=10
listeners=PLAINTEXT://192.168.227.128:9092
log.dirs=/tmp/kafka1
num.partitions=4
min.insync.replicas=2
default.replication.factor=3
offsets.topic.replication.factor=2
transaction.state.log.replication.factor=2
transaction.state.log.min.isr=2
log.flush.interval.messages=10000
log.flush.interval.ms=1000
log.retention.hours=168
blog.segment.bytes=1073741824
zookeeper.connect=localhost:2181,localhost:2182,localhost:2183
group.initial.rebalance.delay.ms=5000
auto.leader.rebalance.enable=true
compression.type=lz4
delete.topic.enable=true
message.max.bytes=1048588
broker.rack=RC1
-----------------------------

cp server1.properties server2.properties
cp server1.properties server3.properties
cp server1.properties server4.propert

vi server2.properties
broker.id=20
listeners=PLAINTEXT://localhost:9093
log.dirs=/tmp/kafka2


vi server3.properties 
broker.id=30
listeners=PLAINTEXT://localhost:9094
log.dirs=/tmp/kafka3
broker.rack=RC2

vi server4.properties 
broker.id=40
listeners=PLAINTEXT://localhost:9095
log.dirs=/tmp/kafka4
broker.rack=RC2

mkdir -p /tmp/kafka1
mkdir -p /tmp/kafka2
mkdir -p /tmp/kafka3
mkdir -p /tmp/kafka4

chmod -R 777 /tmp/kafka1
chmod -R 777 /tmp/kafka2
chmod -R 777 /tmp/kafka3
chmod -R 777 /tmp/kafka4
----------------------------------------------

cd /home/rps/kafka_2.12-3.6.0/bin
 
----------------------- 
nohup ./kafka-server-start.sh ../config/server1.properties &
nohup ./kafka-server-start.sh ../config/server2.properties &
nohup ./kafka-server-start.sh ../config/server3.properties & 
nohup ./kafka-server-start.sh ../config/server4.properties &

// to get to ActiveContoller ID:
./zookeeper-shell.sh localhost:2181 get /controller

//to get all the brokers ids:

./zookeeper-shell.sh  localhost:2181 ls /brokers/ids
--------------------------------------

//to create a topic "demo1"

./kafka-topics.sh --create --topic demo1 --bootstrap-server localhost:9093,localhost:9095

//to list all the topics
./kafka-topics.sh --list --bootstrap-server localhost:9093,localhost:9095

./kafka-topics.sh --describe --topic demo1 --bootstrap-server localhost:9093,localhost:9095

//to discribe the topic
./kafka-topics.sh --create --topic demo2 --partitions 2  --bootstrap-server localhost:9093,localhost:9095

./kafka-topics.sh --create --topic demo3 --partitions 2 --replication-factor 2 --bootstrap-server localhost:9093,localhost:9095 --config max.message.bytes=2096588



./kafka-topics.sh --describe --topic demo2 --bootstrap-server localhost:9093,localhost:9095

./kafka-topics.sh --describe --topic demo3 --bootstrap-server localhost:9093,localhost:9095

./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demo1 --property parse.key=true --property key.separator=:

>1:msg1
>2:msg2
-------------------
./kafka-console-consumer.sh --topic demo1 --bootstrap-server localhost:9092 --partition 3 --offset 0 --property print.key=true --property print.timestamp=true --property print.offset=true

./kafka-console-consumer.sh --topic wc --from-beginning --bootstrap-server 192.168.227.128:9092 --property print.key=true --property print.timestamp=true --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

./kafka-console-consumer.sh --topic numbers-topic --from-beginning -trap-server 192.168.227.128:9092 --property print.key=true --property print.timestamp=true --property value.deserializer=org.apache.kafka.common.serialization.IntegerDeserializer --property key.deserializer=org.apache.kafka.common.serialization.IntegerDeserializer



----------------------------------------


Day 2:

How to disable firewaddD

su root
password: redhat

systemctl disable firewalld
systemctl stop firewalld

How to find and kiill the process

ps -ef|grep server1.properties

kill -9 process_ID

ps -ef|grep server2.properties

kill -9 process_ID


ps -ef|grep server3.properties

kill -9 process_ID

ps -ef|grep server4.properties

kill -9 process_ID

------------------------------------

./kafka-topics.sh --create --topic demo1 --bootstrap-server 192.168.162.128:9092
./kafka-topics.sh --describe --topic demo1 --bootstrap-server 192.168.162.128:9092
./kafka-topics.sh --create --topic demo2  --partitions 2 --replication-factor 2 --bootstrap-server 192.168.162.128:9092
./kafka-topics.sh --describe --topic demo2 --bootstrap-server 192.168.162.128:9092

./kafka-topics.sh --list --bootstrap-server 192.168.162.128:9092

 ./kafka-console-producer.sh --topic demo1 --bootstrap-server 192.168.162.128:9092
 
./kafka-console-consumer.sh --topic demo1 --from-beginning  --bootstrap-server 192.168.162.128:9092 --property print.partition=true --property print.offset=true

./kafka-console-consumer.sh --topic demo1 --partition 3 --offset 0 --bootstrap-server 192.168.162.128:9092


./kafka-topics.sh --create --topic sensor --partitions 10 --bootstrap-server 192.168.227.128:9092

How to disable firewalld on linux?

systemctl disable firewalld
systemctl stop firewalld 

./kafka-configs.sh --entity-type topics --entity-name demo1 --alter --add-config min.insync.replicas=1 --bootstrap-server 192.168.162.128:9092
---------------------------
./kafka-topics.sh --create --topic sensor --partitions 10 --bootstrap-server 192.168.162.128:9092

SensorProducer >> change the IP address and run
---------------------------------------
SensorPartitioner
-------------------------
SupplierProducer>> change the IP address and run

SupplierConsumer >> change the IP address and run
------------------------------------
JsonProducer >> change the IP address and run
JsonConsumer >> change the IP address and run  demo5 grp1

-----------------------------------------------
SimpleConsumer>> demo1: change the IP address and run

SimpleConsumerDup>> demo1change the IP address and run

./kafka-consumer-groups.sh --describe --group walmartJavaApp2 --bootstrap-server 192.168.162.128:9092

/kafka-dump-log.sh --print-data-log --files /tmp/kafka1/demo1-2/00000000000000000000.log



 ./kafka-consumer-groups.sh --describe --group walmartJavaApp2 --bootstrap-server 192.168.227.128:9092


----------

./kafka-consumer-groups.sh --describe --group  walmartJavaApp2 --bootstrap-server 192.168.227.128:9092

./kafka-consumer-groups.sh --topic demo1  --group walmartJavaApp2 --bootstrap-server 192.168.227.128:9092 --reset-offsets -to-offset 2 --execute 

./kafka-consumer-groups.sh --topic demo1  --group walmartJavaApp2 --bootstrap-server 192.168.227.128:9092 --reset-offsets --shift-by 15 --execute



--shift-by <positive_or_negative_integer>
--to-current
--to-latest
--to-offset <offset_integer>
--to-datetime <datetime_string>
--by-duration <duration_string>

---------------------------------

1. First we need mention all the configuration attributes in the below file:

cd /home/rps/kafka_2.12-3.6.0/bin

sudo vi ../config/connect-file-source.properties

name=local-file-source-mukesh
connector.class=FileStreamSource
tasks.max=2
errors.tolerance=all
errors.retry.delay.max.ms=60000
errors.retry.timeout=5000
file=/home/rps/test.txt
topic=connect-test
transforms=MakeMap,InsertSource
transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
transforms.MakeMap.field=record
transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
transforms.InsertSource.static.field=company_name
transforms.InsertSource.static.value=FMR
-------------------------------------------
sudo vi ../config/connect-standalone.properties

bootstrap.servers=192.168.227.128:9092,192.168.227.128:9093
key.converter.schemas.enable=false
value.converter.schemas.enable=false
offset.storage.file.filename=/home/rps/connect.offsets
plugin.path=/home/rps/kafka_2.12-3.6.0/libs
----------------------------------------------
Start the kafka-connect:
cd /home/rps/kafka_2.12-3.6.0/bin

./connect-standalone.sh ../config/connect-standalone.properties ../config/connect-file-source.properties

---------------------------------
echo "message1">> /home/rps/test.txt
echo "message2">> /home/rps/test.txt
echo "message3">> /home/rps/test.txt
-------------------------------------------------------
let's check the topic

./kafka-topics.sh --list --bootstrap-server 192.168.227.128:9092

./kafka-console-consumer.sh --topic connect-test --from-beginning --bootstrap-server 192.168.227.128:9092


-----------------------------------
Confluent :
curl -O http://packages.confluent.io/archive/5.1/confluent-5.1.0-2.11.tar.gz

tar -xvf confluent-5.1.0-2.11.tar.gz

cd /home/rps/confluent-5.1.0/bin

./confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:0.1.0

./confluent start
 
./confluent status

----------
./kafka-topics.sh --create --topic demo12 --partitions 2 --replication-factor 1 --bootstrap-server 192.168.227.128:9092 --config cleanup.policy=compact --config max.compaction.lag.ms=1000

./kafka-console-consumer.sh --topic demo12 --from-beginning --bootstrap-server 192.168.227.128:9092 --property print.key=true --property print.timestamp=true

D. create 2 topics "users" and "pageviews"

./kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic users

./kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic pageviews

E. download the Json file containing configuration for running Java applicationt to publish data into pageview topic

wget https://github.com/confluentinc/kafka-connect-datagen/raw/master/config/connector_pageviews_cos.config

F. Now run kafka-connect using REST API so that it can run java application publising data into pageviews topic in AVRO format

curl -X POST -H "Content-Type: application/json" --data @connector_pageviews_cos.config http://localhost:8083/connectors

G. let's download another json file to write into "users" topic
wget https://github.com/confluentinc/kafka-connect-datagen/raw/master/config/connector_users_cos.config

H. Start the kakfa-connect to publish into "users" topic
curl -X POST -H "Content-Type: application/json" --data @connector_users_cos.config http://localhost:8083/connectors

let's read and see whether data is written:
./kafka-avro-console-consumer --topic pageviews --from-beginning --bootstrap-server localhost:9092

I. 
To verify Schema registry :

curl -X GET http://localhost:8081/subjects

curl -X GET http://localhost:8081/subjects/pageviews-value/versions/1
curl -X GET http://localhost:8081/schemas/ids/61

J. We shall start the KSQL prompt 

sudo ./ksql 

CREATE STREAM pageviews (viewtime BIGINT, userid VARCHAR, pageid VARCHAR) \
WITH (KAFKA_TOPIC='pageviews', VALUE_FORMAT='AVRO');

show streams;
describe PAGEVIEWS;
select * from PAGEVIEWS limit 10;


CREATE TABLE users (registertime BIGINT, gender VARCHAR, regionid VARCHAR,    \
userid VARCHAR, interests array<VARCHAR>, contact_info map<VARCHAR, VARCHAR>) \
WITH (KAFKA_TOPIC='users', VALUE_FORMAT='AVRO', KEY = 'userid');

SET 'auto.offset.reset'='earliest';

CREATE STREAM pageviews_female AS SELECT users.userid AS userid, pageid, \
regionid, gender FROM pageviews LEFT JOIN users ON pageviews.userid = users.userid \
WHERE gender = 'FEMALE';

CREATE STREAM pageviews_female_like_89 WITH (kafka_topic='pageviews_enriched_r8_r9', \
value_format='AVRO') AS SELECT * FROM pageviews_female WHERE regionid LIKE '%_8' OR regionid LIKE '%_9';

select * from PAGEVIEWS  where USERID='User_8' limit 100;

curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" --data '{ "schema": "{ \"type\": \"record\", \"name\": \"Person\", \"namespace\": \"com.ippontech.kafkatutorials\", \"fields\": [ { \"name\": \"firstName\", \"type\": \"string\" }, { \"name\": \"lastName\", \"type\": \"string\" }, { \"name\": \"birthDate\", \"type\": \"long\" } ]}" }' http://localhost:8081/subjects/persons-avro-value/versions

sudo curl -X GET http://localhost:8081/schemas/ids/67

https://fmr.co1.qualtrics.com/jfe/form/SV_aWd4d4pK3bRdEgt


